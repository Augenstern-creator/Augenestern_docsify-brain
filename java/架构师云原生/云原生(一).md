# 1、云原生





## 1.1、专有网络VPC

![](云原生(一).assets/1.png)

每个ECS都有一个公网IP、私有IP，私有IP是固定的不会变动的，并且集群内的各个ECS的通信都是走私有IP的：

- 因为私有IP更快，且不产生流量费用。

- 公网IP之间通信是会产生流量费用的。公网ip也有带宽限制。

> 自己购买的云服务器ECS，其实默认是加入了一个专有网络VPC,私有IP是由这个VPC来给定的。

![](云原生(一).assets/2.png)



一个VPC用16位掩码的话，可以分配65536个私有IP。如上图，192.168.0.0 - 192.168.255.255，并且VPC下我们可以新建交换机来分配可用区，如上图，两个交换机分配了两个可用区AB。

- 可用区A可分配254台ECS的私有IP
- 可用区B可分配254台ECS的私有IP
- 剩余未分配的ip不可用
- **同一VPC下的服务器可互通，同一VPC下的两个不同可用区（不同交换机）的服务器可互通**
- **不同vpc下的服务器在物理层做了网络隔离，不可互通**

![](云原生(一).assets/3.png)



> 推荐:[子网计算](https://www.sojson.com/convert/subnet.html)

## 1.2、Docker

![](云原生(一).assets/4.png)

Docker已经很熟悉了，大致复习下：

1. Docker_Host：安装Docker的主机，也就是宿主机,Linux
2. Registry：镜像仓库，也就是DockerHub
3. Images：镜像，一般将镜像拉取到Linux上，启动运行后变为容器Containers
4. Client：操作Docker主机的客户端（命令行、UI等）

> 整体流程为安装Docker，拉取镜像，运行镜像变为容器

```bash
yum install nginx

whereis nginx

cd /etc/nginx

# 从配置文件可以看出 index.html 是在 /usr/share/nginx/html 下的
cat nginx.conf

systemctl start nginx
```

> 如果访问不通nginx记得开通服务器的防火墙80端口





# 2、Kubernetes

- 读音：**酷博奈忒丝（K8s）**

谈起 k8s 的发展历史，首先回顾一下项目部署的发展历程，分布经历了物理机、虚拟机、容器化三个阶段的演变。

1. **传统部署时代**：早期，应用程序直接在物理服务器上运行，无法为物理服务器中的应用程序定义资源边界，这会导致资源分配问题。例如，如果在物理服务器上运行多个应用程序，则可能会出现一个应用程序占用大部分资源的情况，结果可能导致其他应用程序的性能下降。一种解决方案是在不同的物理服务器上运行每个应用程序，但是由于资源利用不足而无法扩展，并且维护许多物理服务器的成本很高。
2. **虚拟化部署时代**：为了解决物理机存在的弊端，引入了虚拟化技术，支持在单个物理服务器的 CPU 上运行多个虚拟机（VM），每个 VM 是一台完整的计算机，在虚拟化硬件之上运行所有组件，包括其自己的操作系统。虚拟化技术允许应用程序在 VM 之间隔离，提供一定程度的安全，一个应用程序的信息不能被另一应用程序随意访问。虚拟化技术能够更好地利用物理服务器上的资源，由于可轻松地添加或更新应用程序而实现更好的可伸缩性，降低了硬件成本。
3. **容器部署时代**：容器类似于 VM，但是具有被放宽的隔离属性，可以在应用程序之间共享操作系统，因此容器被认为是轻量级的隔离。容器与 VM 类似，具有自己的文件系统、CPU、内存、进程空间等，由于它们与基础架构分离，因此可以跨云和操作系统发行版本进行移植。

> 问题出现：一旦容器多了，如何进行管理？

随着微服务、容器化等技术的发展，解决了资源利用率不高的问题，但是随之而来却是如何进行容器管理，过多的容器使得运维工作也成为了一种负担，因此**容器编排**对于大型依托于容器化部署的分布式系统至关重要。K8s就是一个大规模容器编排系统：

- **服务发现和负载均衡**：Kubernetes 可以使用 DNS 名称或自己的 IP 地址公开容器，如果进入容器的流量很大， Kubernetes 可以负载均衡并分配网络流量，从而使部署稳定。
- **存储编排**：Kubernetes 允许你自动挂载你选择的存储系统，例如本地存储、公共云提供商等。
- **自动部署和回滚**：你可以使用 Kubernetes 描述已部署容器的所需状态，它可以以受控的速率将实际状态更改为期望状态。例如，你可以自动化 Kubernetes 来为你的部署创建新容器， 删除现有容器并将它们的所有资源用于新容器。
- **自动完成装箱计算**：Kubernetes 允许你指定每个容器所需 CPU 和内存（RAM）。 当容器指定了资源请求时，Kubernetes 可以做出更好的决策来管理容器的资源。
- **自我修复**：Kubernetes 重新启动失败的容器、替换容器、杀死不响应用户定义的 运行状况检查的容器，并且在准备好服务之前不将其通告给客户端。
- **密钥与配置管理**：Kubernetes 允许你存储和管理敏感信息，例如密码、OAuth 令牌和 ssh 密钥。 你可以在不重建容器镜像的情况下部署和更新密钥和应用程序配置，也无需在堆栈配置中暴露密钥。



## 2.1、组织架构

- k8s 集群 = 多个 master node + 多个 work node
  - Kubernetes **Cluster** **=** N **Master** Node **+** N **Worker** Node
  - N 主节点 + N 工作节点； N>=1

![](云原生(一).assets/6.png)

​	k8s 通常是集群化部署，一个 k8s 集群由一组被称作节点（Node）的机器组成，一个节点可以理解为一台服务器，这些节点上运行 k8s 所管理的容器化应用。

​	集群具有至少一个**控制节点**（MasterNode）和若干**工作节点**（WorkNode），节点可以是物理机或者虚拟机。

​	**控制节点**包含了**控制平面**（Contro Plane），控制平面中的组件用来管理整个集群，**工作节点**用来托管对应的工作负载 Pod。

- **控制平面**：控制平面可以找到用于控制集群的 k8s 组件以及一些有关集群状态和配置的数据。这些核心 k8s 组件负责处理重要的工作，以确保容器以足够的数量和所需的资源运行。
- **kube-apiserver**：如果需要与您的 k8s 集群进行交互，就要通过 API。k8s API 是 k8s 控制平面的前端，用于处理内部和外部请求。您可以通过 kubeadm 来访问API。
- **kube-scheduler**：您的集群是否状况良好？如果需要新的容器，要将它们放在哪里？这些是 k8s 调度程序所要关注的问题。调度程序会考虑容器集的资源需求（例如 CPU 或内存）以及集群的运行状况。随后，它会将容器集安排到适当的计算节点。
- **kube-controller-manager**：控制器负责实际运行集群，而 k8s 控制器管理器则是将多个控制器功能合而为一。控制器用于查询调度程序，并确保有正确数量的容器集在运行。如果有容器集停止运行，另一个控制器会发现并做出响应。控制器会将服务连接至容器集，以便让请求前往正确的端点。还有一些控制器用于创建帐户和 API 访问令牌。
- **etcd**：配置数据以及有关集群状态的信息位于 etcd（一个KV存储数据库）中。etcd 采用分布式、容错设计，被视为集群的最终事实来源。
- **容器集 Pod**：容器集是 k8s 对象模型中最小、最简单的单元。它代表了应用的单个实例。每个容器集都由一个容器（或一系列紧密耦合的容器）以及若干控制容器运行方式的选件组成。容器集可以连接至持久存储，以运行有状态应用。
- **容器运行时引擎**：为了运行容器，每个计算节点都有一个容器运行时引擎。比如 docker。
- **kubelet**：每个计算节点中都包含一个 kubelet，这是一个与控制平面通信的微型应用。kubelet 可确保容器在容器集内运行。当控制平面需要在节点中执行某个操作时，kubelet 就会执行该操作。
- **kube-proxy**：每个计算节点中还包含 kube-proxy，这是一个用于优化 k8s 网络服务的网络代理。kube-proxy 负责处理集群内部或外部的网络通信。

![](云原生(一).assets/5.png)通俗解释：

- K8s集群就是硅谷集团，集团下有一个Master主节点，也就是我们的**控制平面**（Contro Plane），理解未硅谷总部，还有若干分厂Node，也就是若干Work节点，每个节点就是一台服务器。
- **kube-controller-manager**控制器管理器就是硅谷总部的决策者，**etcd**是集团的资料库，**kube-apiserver**是集团的秘书，负责处理内部和外部请求。**kube-scheduler**是项目的调度者，来负责哪些厂负责哪些项目。
- 每个节点Node都有**kubelet**，理解为厂长，秘书部下发任务给厂长，**kube-proxy**是厂子里面的代理，理解为门卫大爷，负责处理集群内部或外部的网络通信。



## 2.2、一键式安装

中文官网：[https://kubesphere.io/zh/](https://kubesphere.io/zh/)

- 读音：**库伯瑟fai儿~**

1. 文档中心 - 选择最新的版本文档

![](云原生(一).assets/9.png)

2. 我这里选择单节点安装,首先下载KubeKey

![](云原生(一).assets/10.png)



3. 安装完成后，还需要安装依赖项

![](云原生(一).assets/11.png)



```bash
yum install -y socat conntrack ebtables ipset
```

4. 安装完依赖就可以一句话安装 Kubernetes 和 KubeSphere 了

> 注意：hostname必须得规范，建议`hostnamectl set-hostname k8s-master`

```bash
./kk create cluster --with-kubernetes v1.22.12 --with-kubesphere v3.4.1
```

![](云原生(一).assets/12.png)

5. 如果出现报错，`Failed to exec command: sudo -E /bin/bash -c xxxx` 这种，一般是由于您无法从 `dockerhub.io` 下载镜像，这个时候需要为docker切换镜像源
   - 我这里采用毫秒镜像:[毫秒镜像](https://1ms.run/)

```bash
echo '{"registry-mirrors": ["https://docker.1ms.run"]}' | sudo tee /etc/docker/daemon.json > /dev/null
systemctl daemon-reload
systemctl restart docker
```

6. 接着再次执行

```bash
./kk create cluster --with-kubernetes v1.22.12 --with-kubesphere v3.4.1
```

![](云原生(一).assets/13.png)



7. 输入以下命令以检查安装结果。

```bash
kubectl logs -n kubesphere-system $(kubectl get pod -n kubesphere-system -l 'app in (ks-install, ks-installer)' -o jsonpath='{.items[0].metadata.name}') -f
```

输出信息会显示 Web 控制台的 IP 地址和端口号，默认的端口号是 `30880`。现在，您可以使用默认的帐户和密码 (`admin/P@88w0rd`) 通过 `<NodeIP>:30880` 访问控制台

![](云原生(一).assets/14.png)

## 2.3、命令

> Kubectl 是用于控制 kubernetes 集群的命令工具。

语法格式：

```bash
kubectl command type name flages
```

- `command`: 子命令，如 create、get、describe、delete
- `type`：资源类型，可以表示单数、复数或缩写形式
- `name`：资源名称。
- `flags`：指定可选标志，或附加的参数

```bash
# 查看集群所有节点
kubectl get nodes

#根据配置文件，给集群创建资源
kubectl apply -f xxxx.yaml

#查看集群部署了哪些应用？
docker ps   ===   kubectl get pods -A

# 运行中的应用在docker里面叫容器，在k8s里面叫Pod
kubectl get pods -A
```

- 查看集群部署了哪些应用(一个应用就是一个pod):`kubectl get pod -A`

![](云原生(一).assets/15.png)

- 查看集群所有节点:`kubectl get nodes`

![](云原生(一).assets/16.png)





# 3、Kubernetes核心概念

## 3.0、资源创建方式

K8s里面的资源创建有两种方式，第一种是命令行，第二种是通过编写YAML文件，然后使用`kubectl apply -f xxx.yaml` 命令来创建资源。



## 3.1、Namespace

Namespace 用来隔离资源，对集群资源进行隔离划分，默认只隔离资源，不隔离网络。

> Namespace是kubernetes系统中的一种非常重要资源，它的主要作用是用来实现**多套环境的资源隔离**或者**多租户的资源隔离**。

例如K8s集群有三台机器，部署了三个应用。

![](云原生(一).assets/18.png)

- 生产环境可以用一个Namespace -prod
- 生产环境可以用一个Namespace -dev
- 应用A只能引用生产prod下的配置文件，不能引用开发dev下的配置文件。但是整体网络是打通的，当然网络策略也可以设置来达到网络隔离。

### 3.1.1、查看命名空间列表

```bash
# 获取所有名称空间 - 简写
kubectl get ns 
kubectl get namespaces # 全写

#查看namespace
kubectl get namespace
#查看kube-system命名空间下的pod
kubectl get pods -n kube-system
#查看所有namespace下的pod
kubectl get pods -A
```

![](云原生(一).assets/19.png)

kubernetes启动时会创建四个初始命名空间：

- `default`:kubernetes包含这个命名空间，以便你无需创建新的命名空间即可开始使用新集群

- `kube-node-lease`:该命名空间包含用于各个节点关联的Lease(租约)对象。节点租约允许kubelet发送心跳，由此控制能够检测到节点故障。

- `kube-public`:所有的客户端(包括未经身份验证的客户端)都可以读取该命名空间。该命名空间主要预留未集群使用，以便某些资源需要在整个集群中可见可读。该命名空间的公属性是一种约定而非要求。

- `kube-system`:该命名空间使用kubernetes系统创建的对象。

```bash
# #查看所有namespace下的pod
kubectl get pods -A
```

![](云原生(一).assets/20.png)



```bash
# 获取默认default命名空间的运行的应用
kubectl get pods

# 获取kube-system命名空间运行的应用
kubectl get pods -n kube-system
```

![](云原生(一).assets/21.png)



### 3.1.2、创建命名空间

```bash
# 创建hello命名空间
kubectl create ns hello
# 删除hello命名空间
kubectl delete ns hello
```



如果使用`.yaml`来创建，只需新建一个`xx.yaml`，写入如下，然后执行：`kubectl apply -f xx.yaml`

```yaml
# 版本号(固定写法)
apiVersion: v1
# 资源类型(固定写法)
kind: Namespace
# 源数据
metadata:
  name: hello
```

如果要删除，也是使用`.yaml`的方式删除，执行：`kubectl delete -f xx.yaml`

```bash
kubectl apply -f https://example.com/deploy.yaml

kubectl delete -f https://example.com/deploy.yaml
```



> 注意:若在创建资源时不写命名空间，默认放在default命名空间

### 3.1.3、查看命名空间详情

```bash
# your_namespace 为你自己的命名空间名称
kubectl describe ns your_namespace 
```







## 3.2、Pod

Pod是Kubernetes中应用的最小单位。

![](云原生(一).assets/17.png)



我们之前在Docker中运行的是容器，在容器中又加装一层就是Pod。

- 通俗理解：**Docker中的redis容器、mysql容器等可以住在一个Pod宿舍里面。**所以Pod是指运行中的一组容器。
- 这样是K8s为了更好的管理容器，**启动redis容器就变成了启动其所在的Pod**。
- 一句话解释：学生住宿舍，可住单间，可住多人间，管理学生就变为以宿舍为单位管理。

![](云原生(一).assets/15.png)

从上面我们可以看到 READY 字段是 1/1，也就是1个容器住一个Pod。如果是1/2，说明这个Pod有1个容器没在工作。

好！现在我们来定义Pod：

- 由一个容器或多个容器组成(最少一个容器)
- 是k8s中最小的管理元素
- **同一个Pod共享IP及权限,主机名称,共享存储设备,命名空间**



### 3.2.1、获取命名空间下Pod列表

```bash
# your_namespace为你自己的命名空间名称
kubectl get pods -n your_namespace -o wide -w 
```

- -n 指定命名空间 
- -o wide 显示更加详细信息
- -o name 只显示名字
- -o yaml/json  以yaml/json语法格式显示资源对象
- -w 持续监听





### 3.2.2、创建并运行Pod

接下来我们来看下如何创建并运行Pod：

```bash
# kubectl run [pod名称] --image=[镜像]:[镜像版本] --port=[对外端口] --namespace [namespace]

# pod的名字为 mynginx
# 创建的pod默认在default命名空间
kubectl run mynginx --image=nginx

# 查看default名称空间的Pod
kubectl get pods
```

![](云原生(一).assets/22.png)

### 3.2.3、查看Pod详细信息

```bash
# 描述 
# kubectl describe pod your_pod -n your_namespace 
# your_pod为自己的pod名称，your_namespace为你自己的命名空间名称
kubectl describe pod mynginx
```

分配给k8s-master节点去创建mynginx的应用，然后拉取镜像，创建容器等。

![](云原生(一).assets/23.png)

```bash
# 查看pod的运行日志
# kubectl logs pod名字
kubectl logs mynginx
```

![](云原生(一).assets/24.png)

### 3.2.4、进入Pod

```bash
# 查看当前命名空间下所有的Pod的详细信息
kubectl get pod -owide
```

![](云原生(一).assets/25.png)

我们可以看到其中有个字段是IP，Pod 的内部 IP 地址，用于在集群内部进行通信。也就是我们的nginx容器的ip，我们知道nginx的默认端口的80，我们不妨测试一下：

```bash
curl ip:80
```

![](云原生(一).assets/26.png)

我们之前要改这个首页的内容，我们得使用`docker exec` 命令去进入容器的目录中改，现在就变成了：

- `kubectl exec your_pod -c 容器名称 -n your_namespace -it /bin/bash`

```bash
# kubectl exec -it pod名 -- /bin/bash
 kubectl exec -it mynginx -- /bin/bashd
 
 # 退出
 exit
```

![](云原生(一).assets/27.png)

如果我们想要一个pod里面装两个容器,yaml文件如下：

```yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: myapp
  name: myapp
spec:
  containers:
  - image: nginx
    name: nginx
  - image: tomcat
    name: tomcat
```

> 注意：
>
> 1. 同一个Pod里面的容器互相访问，只需要`127.0.0.1:端口号`，其他Pod要访问这个Pod，需要`ip:端口`
>
> ![](云原生(一).assets/28.png)
>
> 2. 同一个Pod里面可以跑两个Nginx容器吗？**不可以，端口被占用**
>
> ![](云原生(一).assets/29.png)





## 3.3、Deployment

Deployment可以控制Pod，使Pod拥有**多副本，自愈，扩缩容**等能力。

### 3.3.1、创建Pod

我们之前是使用：`kubectl run mynginx --image=nginx` 来创建Pod，如果采用Deployment：

```bash
# 具备自愈能力
kubectl create deployment mynginx --image=nginx
```

> 采用`kubectl`来创建容器，具有自愈能力，比如不小心删掉了这个容器，那么k8s会自动创建这个容器。
>
> ```bash
> kubectl delete pod pod名s
> ```

那岂不是没得删了？也可以,删除deploy即可:

```bash
# 查看Deployment创建的pod
kubectl get deploy

# 删除Deploy
kubectl delete deploy mynginx
```

![](云原生(一).assets/30.png)



### 3.3.2、多副本



```bash
# --replicas=3 表示在创建 my-dep 部署时，Kubernetes 会确保始终有 3 个运行中的 nginx Pod 副本
kubectl create deployment my-dep --image=nginx --replicas=3
```

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: my-dep
  name: my-dep
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-dep
  template:
    metadata:
      labels:
        app: my-dep
    spec:
      containers:
      - image: nginx
        name: nginx
```

![](云原生(一).assets/31.png)



### 3.3.3、扩缩容

- 将名为 `my-dep` 的部署的副本数量调整为 5 个。

```bash
# --replicas=5 表示要将目标资源的副本数量调整为 5 个
# deployment 表示要操作的资源类型是部署（Deployment），my-dep 是该部署的名称。
kubectl scale --replicas=5 deployment/my-dep
```













### 3.3.4、自愈和故障转移

- 自愈是指 K8s 集群能够自动检测到集群中的故障或异常，并自动采取措施来恢复到正常状态的能力。

- 故障转移是指在 K8s 集群中，当某个资源（如节点、Pod 或服务）发生故障时，系统能够自动将工作负载转移到其他可用的资源上，以保证应用程序的连续性和可用性。







### 3.3.5、滚动更新

滚动更新是一种逐步替换旧版本 Pod 为新版本 Pod 的更新策略，它允许在更新过程中持续提供服务，避免了传统的全量更新可能导致的服务中断问题。通过滚动更新，Kubernetes 可以在不影响应用程序可用性的前提下，将新的应用版本逐步部署到集群中。

工作原理：

1. **更新触发**：当用户修改 Deployment、StatefulSet 或 DaemonSet 等控制器的配置（如更改容器镜像版本）时，滚动更新过程会被触发。
2. **创建新版本 Pod**：Kubernetes 会根据新的配置创建少量新版本的 Pod。这些新 Pod 会经历健康检查（如存活探针和就绪探针），确保它们能够正常工作。
3. **逐步替换旧版本 Pod**：一旦新版本 Pod 通过健康检查，Kubernetes 会开始逐步删除旧版本的 Pod。同时，会继续创建新的 Pod，直到所有旧版本 Pod 被替换为新版本 Pod。
4. **监控和调整**：在整个更新过程中，Kubernetes 会持续监控 Pod 的状态和应用程序的健康状况。如果出现问题，如新版本 Pod 无法通过健康检查，更新过程会暂停，以避免影响服务的可用性。

```bash
# 更新 my-dep 的nginx镜像版本
kubectl set image deployment/my-dep nginx=nginx:1.17.9 --record
```





### 3.3.6、版本回退

版本回退是指在滚动更新过程中，如果发现新版本存在问题（如应用程序崩溃、性能下降等），可以将应用程序恢复到之前的稳定版本。版本回退机制确保了在更新出现问题时，能够快速恢复服务，减少对业务的影响。

工作原理：

1. **记录版本历史**：Kubernetes 会记录每次更新的版本信息，包括 Deployment 的配置和状态。这些信息被存储在 Deployment 的修订历史中，可以通过 `kubectl rollout history` 命令查看。
2. **触发回退操作**：当需要回退到之前的版本时，用户可以使用 `kubectl rollout undo` 命令指定要回退到的版本。Kubernetes 会根据记录的版本信息，将 Deployment 的配置恢复到指定版本的状态。
3. **执行回退过程**：回退过程与滚动更新类似，Kubernetes 会逐步将新版本的 Pod 替换为旧版本的 Pod，直到所有 Pod 都恢复到之前的稳定版本。

```bash
# 查看 my-dep 的更新历史
kubectl rollout history deployment/my-dep

# 回退到上一个版本
kubectl rollout undo deployment/my-dep

# 回退到指定版本
kubectl rollout undo deployment/my-dep --to-revision=2
```

![](云原生(一).assets/32.png)

> 除了Deployment，k8s还有 `StatefulSet` 、`DaemonSet` 、`Job`  等 类型资源。我们都称为 `工作负载`。有状态应用使用  `StatefulSet`  部署，无状态应用使用 `Deployment` 部署



## 3.4、Service

容器带来的问题：

- 在Pod创建之前,用户无法预知Pod所在节点以及Pod的IP地址
- 一个已经存在的Pod在运行过程中,出现故障,Pod也会在新的节点使用新的IP进行部署
- 应用程序访问服务时,地址是不能经常转变的
- 多个相同的Pod如何访问他们上面的服务

![](云原生(一).assets/33.png)

例如上图三个Pod是Java微服务，前端Vue，前端要发送请求，需要配置后台的地址，显然不可能配置每个Pod的IP地址，而是把一组Pod对外映射为一个Service，这样前端只需要配置Service的ip地址。

```bash
# port信息冒号后面的端口号就是对集群外暴露的访问接口
kubectl expose deployment my-dep --port=8000 --target-port=80
```

- 这条命令的整体作用是将名为 `my-dep` 的部署暴露为一个新的服务。该服务会在端口 8000 上接收外部请求，并将这些请求转发到 `my-dep` 部署中各个 Pod 的端口 80 上，从而实现对部署中应用程序的访问。
- 每次访问Service，Service会**负载均衡**将请求分配到下面三个nginx容器中。

> 每个pod都有一个app标签，
>
> ```bash
> kubectl get pods --show-labels
> ```
>
> ![](云原生(一).assets/34.png)
>
> 之前暴露Service的命令，**也就是将app标签为my-dep的一组pod暴露为一个Service**。
>
> - 当然也是可以使用命令检索app=my-dep的所有pod
>
> ```bash
> #使用标签检索Pod
> kubectl get pod -l app=my-dep
> ```

Service服务原理：

- 自动感知: 服务会创建一个clusterIP,对应资源地址,不管Pod如何变化,服务总能找到对应的Pod,且clusterIP保持不变
- 负载均衡: 若服务器后端对应多个Pod,则会通过IPTables/LVS规则 访问请求最终映射到Pod容器内部,自动实现多个容器的负载均衡
- 自动发现: 服务创建时会自动在内部dns上注册域名

Service可以看作是一组同类Pod**对外的访问接口**。借助Service，应用可以方便地实现服务发现和负载均衡。





### 3.4.1、查看Service

```bash
kubectl get service
```

![](云原生(一).assets/35.png)

我们直接`curl ClusterIP:8000`是可以访问到nginx的，也可以使用nginx容器进行访问Service,也是可以访问的：

```bash
# Serviceip访问
curl Serviceip:8000

# Service域名访问(也是可以访问的)
curl my-dep.default.svc:8000
```

![](云原生(一).assets/36.png)

![](云原生(一).assets/37.png)



### 3.4.2、删除Service

```bash
kubectl delete service my-dep
```



### 3.4.3、ClusterIP

**Service有4种类型：**

- ClusterIP：默认值，它是Kubernetes系统自动分配的虚拟IP，**只能在集群内部访问**
- NodePort：将Service通过指定的Node上的端口暴露给外部，通过此方法，**就可以在集群外部访问服务**
- LoadBalancer：使用外接负载均衡器完成到服务的负载分发，注意此模式需要外部云环境支持
- ExternalName： 把集群外部的服务引入集群内部，直接使用

我们之前使用的命令其实就等同于

```bash
# 将app=my-dep的pod对外暴露为
kubectl expose deployment my-dep --port=8000 --target-port=80 

# 等同于
kubectl expose deployment my-dep --port=8000 --target-port=80 --type=ClusterIP
```

yaml格式如下：

```yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: my-dep
  name: my-dep
spec:
  ports:
  - port: 8000
    protocol: TCP
    targetPort: 80
  selector:
    app: my-dep
  type: ClusterIP
```







### 3.4.4、NodePort

```bash
kubectl expose deployment my-dep --port=8000 --target-port=80 --type=NodePort
```

![](云原生(一).assets/38.png)

- 我们可以发现多暴露了一个30706的端口，这个就是NodePort，相当于给每个Pod都开放了一个30706的端口。
  - 图中是30948端口，**NodePort范围在 30000-32767 之间**

![](云原生(一).assets/39.png)

我们可以直接在集群外(浏览器)访问：`公网ip(宿主机ip/节点ip):8000`

![](云原生(一).assets/40.png)



> 也就是给每台机器都暴露了8000端口，后面负载均衡挂载了nginx。



## 3.5、Ingress

Ingress是Service的统一网关入口，Service是Pod的统一网关入口。可以理解为请求先到Ingress，再到Service，再到Pod。

> - 官网：https://kubernetes.github.io/ingress-nginx/deploy/
> - Github：https://github.com/kubernetes/ingress-nginx

![](云原生(一).assets/41.png)

官网的目前是v1.12.0版本的yaml,我们可以修改为`1.14.0`:

```bash
https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.12.0/deploy/static/provider/cloud/deploy.yaml

# 修改
https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.4.0/deploy/static/provider/cloud/deploy.yaml
```

![](云原生(一).assets/image-20250307001303112.png)

当然这个`deploy.yaml`文件也可以在官方Github仓库中找到:

![image-20250307001400032](云原生(一).assets/image-20250307001400032.png)



### 3.5.1、镜像更换

- 我这里记录一下，首先查看kubectl的版本：

```bash
kubectl version
```

得到

```bash
Client Version: version.Info{Major:"1", Minor:"22", GitVersion:"v1.22.12", GitCommit:"b058e1760c79f46a834ba59bd7a3486ecf28237d", GitTreeState:"clean", BuildDate:"2022-07-13T14:59:18Z", GoVersion:"go1.16.15", Compiler:"gc", Platform:"linux/amd64"}

Server Version: version.Info{Major:"1", Minor:"22", GitVersion:"v1.22.12", GitCommit:"b058e1760c79f46a834ba59bd7a3486ecf28237d", GitTreeState:"clean", BuildDate:"2022-07-13T14:53:39Z", GoVersion:"go1.16.15", Compiler:"gc", Platform:"linux/amd64"}
```

- Client Version代表kubectl的版本，这里是v1.22.12
- Server Version代表master节点的k8s版本，这里是v1.22.12

我们在Github查看一下版本对应：[Ingress-Nginx](https://github.com/kubernetes/ingress-nginx/tree/controller-v1.6.4?spm=a2c6h.12873639.article-detail.4.715a3cb8PxJ4HL&file=controller-v1.6.4)

![](云原生(一).assets/44.png)





```bash
# ingress安装完成后会有 ingress-nginx 命名空间
kubectl get ns

# 安装完成后也会有新的pod和新的service 
kubectl get pod,svc -n ingress-nginx
```

![](云原生(一).assets/42.png)

> 注意记得防火墙放行Ingress的端口：
>
> ![](云原生(一).assets/43.png)
>
> - 32096对应的是HTTP的80端口，32672对应的是HTTPS的443端口。













- https://blog.csdn.net/qq_22075913/article/details/145326585
- https://blog.csdn.net/2301_78183285/article/details/138656873















## 3.6、Helm

 Helm 是 Kubernetes 的包管理器。简化了Kubernetes应用的安装和管理。可以理解为是Linux的yum工具。

- Helm 定义了一种应用程序包格式称为图表charts。一个图表是一组描述Kubernetes资源的文件集合。
- Helm是Kubernetes生态系统中的核心工具之一，它大幅简化了Kubernetes应用的部署和管理流程。

> [Helm版本对应](https://helm.sh/zh/docs/intro/quickstart/)

我们要安装Helm，需要我们已经把Kubernetes装好后，我这里是Kubernetes v1.22.12，可以看到是Helm的3.10.x版本支持。

![image-20250305231056246](云原生(一).assets/image-20250305231056246.png)

1. [Helm 3.10.3版本下载](https://github.com/helm/helm/releases?expanded=true&page=2&q=3.10)，可以手动下载下来上传，也可以curl

```bash
curl -s https://get.helm.sh/helm-v3.10.3-linux-amd64.tar.gz
```



![image-20250305233506776](云原生(一).assets/image-20250305233506776.png)

2. 上传到某个特定目录，进行解压

```bash
tar -zxvf helm-v3.10.3-linux-amd64.tar.gz 

ls

cd linux-amd64

ls

# 查看环境变量
echo $PATH

# 移动到环境变量
mv helm  /usr/local/bin


# 询问是否需要覆盖，输入yes
```

![image-20250306001151032](云原生(一).assets/image-20250306001151032.png)





![image-20250305235126763](云原生(一).assets/image-20250305235126763.png)

> `helm version` 确保安装成功！

3. 添加仓库，Helm V3不再自带默认仓库。需要添加官方仓库或者其他仓库(不添加仓库等于yum没有对应的repo文件)



![image-20250305235322518](云原生(一).assets/image-20250305235322518.png)

4. 进入 [ArtifactHub](https://artifacthub.io/packages/search?kind=0),例如搜索Ingress-nginx仓库

![image-20250305235436251](云原生(一).assets/image-20250305235436251.png)



5. 比如添加Ingress仓库

```bash
helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
helm repo update
```

![image-20250306001251995](云原生(一).assets/image-20250306001251995.png)

查看我们安装的仓库列表：`helm repo list`

![image-20250306001658821](云原生(一).assets/image-20250306001658821.png)

```bash
export http_proxy=http://192.168.100.100:7890
```

6. 安装好仓库就可以使用Helm来安装Ingress

```bash
# 创建ingress-nginx命名空间
kubectl create ns ingress-nginx
# 在ingress-nginx命名空间中安装一个名为ingress的ingress
helm install ingress ingress-nginx/ingress-nginx -n ingress-nginx
```

> 这个对网络有一定要求，可能会装不成功。

简单来说Helm实际上只是Linux的的二进制命令，它需要配合仓库配置才能使用，类似yum命令。就可以完成当前集群一键安装升级卸载应用的功能。



### 3.6.1、🔥增加Helm源

> - 官方文档：[Helm Charts Hub](https://helm-charts.itboon.top/docs/)

由于国内不可抗力原因，这里增加Helm源如下：

```bash
# helm repo add [自定义名字] url
helm repo add stable https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts
helm repo add bitnami "https://helm-charts.itboon.top/bitnami" --force-update
helm repo add grafana "https://helm-charts.itboon.top/grafana" --force-update
helm repo add prometheus-community "https://helm-charts.itboon.top/prometheus-community" --force-update
helm repo add ingress-nginx "https://helm-charts.itboon.top/ingress-nginx" --force-update


# 更新仓库
helm repo update

# 查看仓库列表
helm repo list

# 查看目前安装的
helm list
```

![image-20250306144909639](云原生(一).assets/image-20250306144909639.png)



```bash
# 查找ingress仓库
helm search repo ingress-nginx
# 创建命名空间
kubectl create ns ingress-nagin
# 安装ingress
helm install ingress ingress-nginx/ingress-nginx -n ingress-nginx
```





























