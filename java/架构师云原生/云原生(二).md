

# 0、Yaml文件详情

Kubernetes 支持 Yaml 格式管理资源对象。

## 0.1、Pod yaml

```yaml
apiVersion: v1			#必选，版本号，例如v1
kind: Pod				#必选，Pod
metadata:				#必选，元数据
  name: string			  #必选，Pod名称
  namespace: string		  #必选，Pod所属的命名空间
  labels:				  #自定义标签
    - name: string		    #自定义标签名字
  annotations:			    #自定义注释列表
    - name: string
spec:					#必选，Pod中容器的详细定义
  containers:			  #必选，Pod中容器列表
  - name: string		    #必选，容器名称
    image: string		    #必选，容器的镜像名称
    imagePullPolicy: [Always | Never | IfNotPresent]	#获取镜像的策略：Alawys表示总是下载镜像，IfnotPresent表示优先使用本地镜像，否则下载镜像，Nerver表示仅使用本地镜像
    command: [string]		#容器的启动命令列表，如不指定，使用打包时使用的启动命令
    args: [string]			#容器的启动命令参数列表
    workingDir: string		#容器的工作目录
    volumeMounts:			#挂载到容器内部的存储卷配置
    - name: string			  #引用pod定义的共享存储卷的名称，需用volumes[]部分定义的的卷名
      mountPath: string		  #存储卷在容器内mount的绝对路径，应少于512字符
      readOnly: boolean		  #是否为只读模式
    ports:					#需要暴露的端口库号列表
    - name: string			  #端口号名称
      containerPort: int	  #容器需要监听的端口号
      hostPort: int			  #容器所在主机需要监听的端口号，默认与Container相同
      protocol: string		  #端口协议，支持TCP和UDP，默认TCP
    env:					#容器运行前需设置的环境变量列表
    - name: string			  #环境变量名称
      value: string			  #环境变量的值
    resources:				#资源限制和请求的设置
      limits:				  #资源限制的设置
        cpu: string			    #Cpu的限制，单位为core数，将用于docker run --cpu-shares参数
        memory: string			#内存限制，单位可以为Mib/Gib，将用于docker run --memory参数
      requests:				  #资源请求的设置
        cpu: string			    #Cpu请求，容器启动的初始可用数量
        memory: string		    #内存清楚，容器启动的初始可用数量
    livenessProbe:     		#对Pod内个容器健康检查的设置，当探测无响应几次后将自动重启该容器，检查方法有exec、httpGet和tcpSocket，对一个容器只需设置其中一种方法即可
      exec:					#对Pod容器内检查方式设置为exec方式
        command: [string]	  #exec方式需要制定的命令或脚本
      httpGet:				#对Pod内个容器健康检查方法设置为HttpGet，需要制定Path、port
        path: string
        port: number
        host: string
        scheme: string
        HttpHeaders:
        - name: string
          value: string
      tcpSocket:			#对Pod内个容器健康检查方式设置为tcpSocket方式
         port: number
       initialDelaySeconds: 0	#容器启动完成后首次探测的时间，单位为秒
       timeoutSeconds: 0		#对容器健康检查探测等待响应的超时时间，单位秒，默认1秒
       periodSeconds: 0			#对容器监控检查的定期探测时间设置，单位秒，默认10秒一次
       successThreshold: 0
       failureThreshold: 0
       securityContext:
         privileged:false
    restartPolicy: [Always | Never | OnFailure]		#Pod的重启策略，Always表示一旦不管以何种方式终止运行，kubelet都将重启，OnFailure表示只有Pod以非0退出码退出才重启，Nerver表示不再重启该Pod
    nodeSelector: obeject		#设置NodeSelector表示将该Pod调度到包含这个label的node上，以key：value的格式指定
    imagePullSecrets:			#Pull镜像时使用的secret名称，以key：secretkey格式指定
    - name: string
    hostNetwork:false			#是否使用主机网络模式，默认为false，如果设置为true，表示使用宿主机网络
    volumes:					#在该pod上定义共享存储卷列表
    - name: string				  #共享存储卷名称 （volumes类型有很多种）
      emptyDir: {}				  #类型为emtyDir的存储卷，与Pod同生命周期的一个临时目录。为空值
      hostPath: string			  #类型为hostPath的存储卷，表示挂载Pod所在宿主机的目录
        path: string			    #Pod所在宿主机的目录，将被用于同期中mount的目录
      secret:					#类型为secret的存储卷，挂载集群与定义的secre对象到容器内部
        scretname: string  
        items:     
        - key: string
          path: string
      configMap:				#类型为configMap的存储卷，挂载预定义的configMap对象到容器内部
        name: string
        items:
        - key: string
```



- 创建资源对象：kubectl create -f nginx-deployment.yaml
- 查看创建的资源Pod：kubectl get pod



## 0.2、deployment.yaml

```yaml
apiVersion: extensions/v1beta1   #接口版本
kind: Deployment                 #接口类型
metadata:
  name: cango-demo               #Deployment名称
  namespace: cango-prd           #命名空间
  labels:
    app: cango-demo              #标签
spec:
  replicas: 3
  strategy:
    rollingUpdate:  ##由于replicas为3,则整个升级,pod个数在2-4个之间
      maxSurge: 1      #滚动升级时会先启动1个pod
      maxUnavailable: 1 #滚动升级时允许的最大Unavailable的pod个数
  template:         
    metadata:
      labels:
        app: cango-demo  #模板名称必填
    sepc: #定义容器模板，该模板可以包含多个容器
      containers:                                                                   
        - name: cango-demo                                                           #镜像名称
          image: swr.cn-east-2.myhuaweicloud.com/cango-prd/cango-demo:0.0.1-SNAPSHOT #镜像地址
          command: [ "/bin/sh","-c","cat /etc/config/path/to/special-key" ]    #启动命令
          args:                                                                #启动参数
            - '-storage.local.retention=$(STORAGE_RETENTION)'
            - '-storage.local.memory-chunks=$(STORAGE_MEMORY_CHUNKS)'
            - '-config.file=/etc/prometheus/prometheus.yml'
            - '-alertmanager.url=http://alertmanager:9093/alertmanager'
            - '-web.external-url=$(EXTERNAL_URL)'
    #如果command和args均没有写，那么用Docker默认的配置。
    #如果command写了，但args没有写，那么Docker默认的配置会被忽略而且仅仅执行.yaml文件的command（不带任何参数的）。
    #如果command没写，但args写了，那么Docker默认配置的ENTRYPOINT的命令行会被执行，但是调用的参数是.yaml中的args。
    #如果如果command和args都写了，那么Docker默认的配置被忽略，使用.yaml的配置。
          imagePullPolicy: IfNotPresent  #如果不存在则拉取
          livenessProbe:       #表示container是否处于live状态。如果LivenessProbe失败，LivenessProbe将会通知kubelet对应的container不健康了。随后kubelet将kill掉container，并根据RestarPolicy进行进一步的操作。默认情况下LivenessProbe在第一次检测之前初始化值为Success，如果container没有提供LivenessProbe，则也认为是Success；
            httpGet:
              path: /health #如果没有心跳检测接口就为/
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60 ##启动后延时多久开始运行检测
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 5
          readinessProbe:
            httpGet:
              path: /health #如果没有心跳检测接口就为/
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 30 ##启动后延时多久开始运行检测
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 5
          resources:              ##CPU内存限制
            requests:
              cpu: 2
              memory: 2048Mi
            limits:
              cpu: 2
              memory: 2048Mi
          env:                    ##通过环境变量的方式，直接传递pod=自定义Linux OS环境变量
            - name: LOCAL_KEY     #本地Key
              value: value
            - name: CONFIG_MAP_KEY  #局策略可使用configMap的配置Key，
              valueFrom:
                configMapKeyRef:
                  name: special-config   #configmap中找到name为special-config
                  key: special.type      #找到name为special-config里data下的key
          ports:
            - name: http
              containerPort: 8080 #对service暴露端口
          volumeMounts:     #挂载volumes中定义的磁盘
          - name: log-cache
            mount: /tmp/log
          - name: sdb       #普通用法，该卷跟随容器销毁，挂载一个目录
            mountPath: /data/media    
          - name: nfs-client-root    #直接挂载硬盘方法，如挂载下面的nfs目录到/mnt/nfs
            mountPath: /mnt/nfs
          - name: example-volume-config  #高级用法第1种，将ConfigMap的log-script,backup-script分别挂载到/etc/config目录下的一个相对路径path/to/...下，如果存在同名文件，直接覆盖。
            mountPath: /etc/config       
          - name: rbd-pvc                #高级用法第2中，挂载PVC(PresistentVolumeClaim)
 
#使用volume将ConfigMap作为文件或目录直接挂载，其中每一个key-value键值对都会生成一个文件，key为文件名，value为内容，
  volumes:  # 定义磁盘给上面volumeMounts挂载
  - name: log-cache
    emptyDir: {}
  - name: sdb  #挂载宿主机上面的目录
    hostPath:
      path: /any/path/it/will/be/replaced
  - name: example-volume-config  # 供ConfigMap文件内容到指定路径使用
    configMap:
      name: example-volume-config  #ConfigMap中名称
      items:
      - key: log-script           #ConfigMap中的Key
        path: path/to/log-script  #指定目录下的一个相对路径path/to/log-script
      - key: backup-script        #ConfigMap中的Key
        path: path/to/backup-script  #指定目录下的一个相对路径path/to/backup-script
  - name: nfs-client-root         #供挂载NFS存储类型
    nfs:
      server: 10.42.0.55          #NFS服务器地址
      path: /opt/public           #showmount -e 看一下路径
  - name: rbd-pvc                 #挂载PVC磁盘
    persistentVolumeClaim:
      claimName: rbd-pvc1         #挂载已经申请的pvc磁盘

```



## 0.3、Service yaml

```yaml
apiVersion: v1
kind: Service
matadata:                                #元数据
  name: string                           #service的名称
  namespace: string                      #命名空间  
  labels:                                #自定义标签属性列表
    - name: string
  annotations:                           #自定义注解属性列表  
    - name: string
spec:                                    #详细描述
  selector: []                           #label selector配置，将选择具有label标签的Pod作为管理 
                                         #范围
  type: string                           #service的类型，指定service的访问方式，默认为 
                                         #clusterIp
  clusterIP: string                      #虚拟服务地址      
  sessionAffinity: string                #是否支持session
  ports:                                 #service需要暴露的端口列表
  - name: string                         #端口名称
    protocol: string                     #端口协议，支持TCP和UDP，默认TCP
    port: int                            #服务监听的端口号
    targetPort: int                      #需要转发到后端Pod的端口号
    nodePort: int                        #当type = NodePort时，指定映射到物理机的端口号
  status:                                #当spce.type=LoadBalancer时，设置外部负载均衡器的地址
    loadBalancer:                        #外部负载均衡器    
      ingress:                           #外部负载均衡器 
        ip: string                       #外部负载均衡器的Ip地址值
        hostname: string                 #外部负载均衡器的主机名
```







# 1、存储抽象

每个Pod都会在宿主机上进行数据卷挂载，这样修改宿主机的数据卷，就可以修改Pod里面的内容。这是之前Docker的思想，但是在K8s里面，如果node1机器的pod宕机了，K8s根据故障转移，将宕机的pod在node2机器启动，可是这个pod的挂载目录还依然在node1机器呢！

所以K8s为了解决如上问题，将所有的pod挂载统一为存储层。我们可以在master节点搭建一个NFS文件存储系统作为NFS的服务端，Node1、Node2也搭建NFS作为NFS的客户端，当NFS的服务端数据发生变化，客户端会自动同步。

![](云原生(二).assets/1.png)



## 1.1、安装NFS

```bash
# 所有节点安装NFS
yum install -y nfs-utils
```

```bash
# 主节点master
echo "/nfs/data/ *(insecure,rw,sync,no_root_squash)" > /etc/exports
```

- 将`/nfs/data/`目录设置为共享目录，意味着该目录下的文件和子目录将可以被其他主机通过网络访问
- **`*`**：表示允许所有的主机访问这个共享目录。也可以指定具体的 IP 地址或网段，如`192.168.1.0/24`表示允许该网段内的主机访问。
- **`insecure`**：允许从客户端来的非授权访问，一般用于客户端可能使用大于 1024 的端口进行 NFS 连接的情况。
- **`rw`**：表示该共享目录对客户端来说是可读可写的，客户端可以在这个共享目录中进行读取和写入文件等操作。
- **`sync`**：表示数据同步写入磁盘，保证数据的一致性和完整性，即数据会先写入到磁盘，然后再返回操作成功的信号，这样可以避免数据丢失，但可能会影响性能。
- **`no_root_squash`**：默认情况下，当客户端以 root 用户访问 NFS 共享时，服务器会将其映射为一个非 root 用户，以增强安全性。而`no_root_squash`则表示不进行这种映射，客户端的 root 用户在访问共享目录时将具有 root 权限，这可能会带来一定的安全风险，但在某些特定的场景下可能是必要的。

### 1.1.1、挂载NFS	

```bash
# 主节点
mkdir -p /nfs/data
# 将 rpcbind 服务设置为开机自启，并且立即启动该服务。
systemctl enable rpcbind --now
# 将 nfs-server 服务设置为开机自启，并且立即启动该服务。
systemctl enable nfs-server --now
#配置生效
exportfs -r
```

```bash
# 从节点
# 查看master哪个目录是共享目录
showmount -e Master节点ip

#执行以下命令挂载 nfs 服务器上的共享目录到本机路径 /root/nfsmount
mkdir -p /nfs/data
mount -t nfs Master节点ip:/nfs/data /nfs/data

# 写入一个测试文件
echo "hello nfs server" > /nfs/data/test.txt
```



### 1.1.2、使用yaml挂载NFS

![](云原生(二).assets/2.png)

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx-pv-demo
  name: nginx-pv-demo
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx-pv-demo
  template:
    metadata:
      labels:
        app: nginx-pv-demo
    spec:
      containers:
      - image: nginx
        name: nginx
        # 卷挂载
        volumeMounts:
        # nginx里面的 /usr/share/nginx/html 目录挂载到宿主机上，名字叫html，对应下方的 volumes
        - name: html
          mountPath: /usr/share/nginx/html
      volumes:
        - name: html
         # 名叫html的以nfs挂载方式挂载
          nfs:
            server: master节点的ip
            path: /nfs/data/nginx-pv
```



- 其实也就是 nginx 容器的 /usr/share/nginx/html 路径挂载到 master 节点的 /nfs/data/nginx-pv

> 注意要在master节点先创建`/nfs/data/nginx-pv` 目录





## 1.2、PV和PVC

我们上面NFS共享目录的方式其实存在两个问题：

1. master节点的共享目录需要自己提前手动创建
2. 如果nginx pod 删除了后，它挂载的目录(即` /nfs/data/nginx-pv`)目录不会自动删除。

所以K8S还有另一种方式可以进行挂载：PV和PVC

- *PV：持久卷（Persistent Volume），将应用需要持久化的数据保存到指定位置*
- *PVC：持久卷申明（**Persistent Volume Claim**），申明需要使用的持久卷规格，比如给持久卷20MB*

![](云原生(二).assets/3.png)



我们可以先在机器上提前创建许多持久卷PV，这些持久卷PV统一称为PV池。

- 当某个Pod需要挂载`/data`目录到持久卷，先写一份PVC申请，申请需要多大的持久卷，然后K8s再根据申请书给它分配最合适的。
- 当Pod发生故障转移，其PVC申请不变，依然会给它自己对应的PV
- 当Pod删除掉后，其PVC也会被删除，连带着申请的PV也会被删除。

创建PV：

```bash
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv01-10m
spec:
  capacity:
   # 申请10M的PV
    storage: 10M
  accessModes:
   # 可读可写
    - ReadWriteMany
  storageClassName: nfs
  nfs:
    path: /nfs/data/01
    server: 节点ip
---

apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv02-1gi
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteMany
  storageClassName: nfs
  nfs:
    path: /nfs/data/02
    server: 节点ip
---

apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv03-3gi
spec:
  capacity:
    storage: 3Gi
  accessModes:
    - ReadWriteMany
  storageClassName: nfs
  nfs:
    path: /nfs/data/03
    server: 节点ip
```

- 创建PVC

```yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: nginx-pvc
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      # 申请200MB的PV
      storage: 200Mi
  # 对应创建PV的    storageClassName 名
  storageClassName: nfs
```

当然在申请之前可以使用`kubectl get pv`来查看当前有多少和多大的PV。

- 🔥创建Pod绑定PVC:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx-deploy-pvc
  name: nginx-deploy-pvc
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx-deploy-pvc
  template:
    metadata:
      labels:
        app: nginx-deploy-pvc
    spec:
      containers:
      - image: nginx
        name: nginx
        volumeMounts:
        - name: html
          mountPath: /usr/share/nginx/html
      volumes:
        - name: html
          # 使用nginx-pvc的申请书(我们上面申请PVC的名字是nginx-pvc)
          persistentVolumeClaim:
            claimName: nginx-pvc
```

也可以使用`kubectl get pvc`来查看申请书。





## 1.3、ConfigMap

上面的都是挂载目录，可是如果我们要挂载文件怎么办？比如要把redis的conf文件挂载出来，这个时候使用ConfigMap最好：

```bash
# 创建一个ConfigMap，名字为 redis-conf
# --from-file 是一个选项，用于指定从文件中读取数据并将其添加到 ConfigMap 中
kubectl create cm redis-conf --from-file=redis.conf

# 获取配置集ConfigMap(cm其实都是存储在etcd里面)
kubectl get cm
```

yaml方式如下：

```yaml
apiVersion: v1
data:    #data是所有真正的数据，key：默认是文件名   value：配置文件的内容
  redis.conf: |
    appendonly yes
kind: ConfigMap
metadata:
  name: redis-conf
  namespace: default
```

- `|` 是 YAML 中的多行文本标识符号，它表示后面的内容是多行文本，并且会保留文本中的换行符。

创建Pod时挂载文件：

```yaml
# 指定 Kubernetes API 的版本，v1 是核心 API 版本，适用于基础资源如 Pod
apiVersion: v1
# 定义资源的类型，这里是 Pod，Pod 是 Kubernetes 中最小的可部署单元
kind: Pod
# 元数据部分，包含 Pod 的一些标识信息
metadata:
  # Pod 的名称，用于在 Kubernetes 集群中唯一标识这个 Pod
  name: redis
# Pod 的规格部分，描述了 Pod 的具体配置
spec:
  # 定义 Pod 内的容器列表，一个 Pod 可以包含多个容器
  containers:
  # 第一个容器的配置
  - name: redis
    # 指定容器使用的镜像，这里使用官方的 Redis 镜像
    image: redis
    # 容器启动时执行的命令
    command:
      # 启动 Redis 服务器
      - redis-server
      # 指定 Redis 服务器使用的配置文件路径，这里是容器内部的路径
      - "/redis-master/redis.conf"  #指的是redis容器内部的位置
    # 容器暴露的端口列表
    ports:
    # 定义容器内部监听的端口
    - containerPort: 6379
    # 容器内的挂载点配置，用于将卷挂载到容器内的指定路径
    volumeMounts:
    # 第一个挂载点
    - mountPath: /data
      # 对应的卷名称，用于关联到下面 volumes 部分定义的卷
      name: data
    # 第二个挂载点
    - mountPath: /redis-master
      # 对应的卷名称，用于关联到下面 volumes 部分定义的卷
      name: config
  # 定义 Pod 中使用的卷列表，卷用于在容器之间共享数据或持久化数据
  volumes:
  # 第一个卷的配置
  - name: data
    # 使用 emptyDir 类型的卷，这是一个临时存储卷，Pod 存在期间数据会保留
    emptyDir: {}
  # 第二个卷的配置
  - name: config
    # 使用 ConfigMap 类型的卷，用于将 ConfigMap 中的数据挂载到容器中
    configMap:
      # 指定要使用的 ConfigMap 的名称
      name: redis-conf
      # 定义要从 ConfigMap 中获取的具体项
      items:
      # 从 ConfigMap 中获取键为 redis.conf 的数据
      - key: redis.conf
        # 将获取的数据挂载到容器内的指定路径
        path: redis.conf
```

这个 YAML 文件定义了一个名为 `redis` 的 Pod，其中包含一个 Redis 容器。该容器使用 `redis` 镜像，启动时使用 `/redis-master/redis.conf` 作为配置文件，并监听 6379 端口。

容器挂载了两个卷，一个是 `emptyDir` 类型的卷用于临时存储数据，另一个是 `ConfigMap` 类型的卷，用于将名为 `redis-conf` 的 ConfigMap 中的 `redis.conf` 文件挂载到容器内的 `/redis-master` 目录下。







## 1.4、Secret

Secret 对象类型用来保存敏感信息，例如**密码、OAuth 令牌和 SSH 密钥**。 将这些信息放在 secret 中比放在 Pod 的定义或者容器镜像中来说更加安全和灵活。































